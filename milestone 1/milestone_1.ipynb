{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1: preprocessing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:30.792711Z",
     "start_time": "2024-11-09T19:30:28.984611Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import stanza"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:30.925077Z",
     "start_time": "2024-11-09T19:30:30.805772Z"
    }
   },
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/henry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/henry/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.112967Z",
     "start_time": "2024-11-09T19:30:31.003270Z"
    }
   },
   "source": [
    "data = pd.read_csv('../data/edos_labelled_individual_annotations.csv')\n",
    "print(\"Columns:\", list(data.columns))\n",
    "print(\"Shape:\", data.shape)\n",
    "data.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['rewire_id', 'text', 'annotator', 'label_sexist', 'label_category', 'label_vector', 'split']\n",
      "Shape: (60000, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              rewire_id                                               text  \\\n",
       "0  sexism2022_english-0  [USER] I wonder what keeps that witch looking ...   \n",
       "1  sexism2022_english-0  [USER] I wonder what keeps that witch looking ...   \n",
       "2  sexism2022_english-0  [USER] I wonder what keeps that witch looking ...   \n",
       "3  sexism2022_english-1  What do you guys think about female \"incels\"? ...   \n",
       "4  sexism2022_english-1  What do you guys think about female \"incels\"? ...   \n",
       "\n",
       "   annotator label_sexist label_category                        label_vector  \\\n",
       "0         17       sexist  2. derogation  2.2 aggressive and emotive attacks   \n",
       "1          2       sexist  2. derogation  2.2 aggressive and emotive attacks   \n",
       "2          6   not sexist           none                                none   \n",
       "3         17   not sexist           none                                none   \n",
       "4         15   not sexist           none                                none   \n",
       "\n",
       "   split  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotator</th>\n",
       "      <th>label_sexist</th>\n",
       "      <th>label_category</th>\n",
       "      <th>label_vector</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-0</td>\n",
       "      <td>[USER] I wonder what keeps that witch looking ...</td>\n",
       "      <td>17</td>\n",
       "      <td>sexist</td>\n",
       "      <td>2. derogation</td>\n",
       "      <td>2.2 aggressive and emotive attacks</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-0</td>\n",
       "      <td>[USER] I wonder what keeps that witch looking ...</td>\n",
       "      <td>2</td>\n",
       "      <td>sexist</td>\n",
       "      <td>2. derogation</td>\n",
       "      <td>2.2 aggressive and emotive attacks</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-0</td>\n",
       "      <td>[USER] I wonder what keeps that witch looking ...</td>\n",
       "      <td>6</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-1</td>\n",
       "      <td>What do you guys think about female \"incels\"? ...</td>\n",
       "      <td>17</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-1</td>\n",
       "      <td>What do you guys think about female \"incels\"? ...</td>\n",
       "      <td>15</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a more fine-grained sexism detection, but we're working only with the `label_sexist`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.173566Z",
     "start_time": "2024-11-09T19:30:31.166062Z"
    }
   },
   "source": [
    "data = data.drop(columns=['label_category', 'label_vector'])\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              rewire_id                                               text  \\\n",
       "0  sexism2022_english-0  [USER] I wonder what keeps that witch looking ...   \n",
       "1  sexism2022_english-0  [USER] I wonder what keeps that witch looking ...   \n",
       "2  sexism2022_english-0  [USER] I wonder what keeps that witch looking ...   \n",
       "3  sexism2022_english-1  What do you guys think about female \"incels\"? ...   \n",
       "4  sexism2022_english-1  What do you guys think about female \"incels\"? ...   \n",
       "\n",
       "   annotator label_sexist  split  \n",
       "0         17       sexist  train  \n",
       "1          2       sexist  train  \n",
       "2          6   not sexist  train  \n",
       "3         17   not sexist  train  \n",
       "4         15   not sexist  train  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotator</th>\n",
       "      <th>label_sexist</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-0</td>\n",
       "      <td>[USER] I wonder what keeps that witch looking ...</td>\n",
       "      <td>17</td>\n",
       "      <td>sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-0</td>\n",
       "      <td>[USER] I wonder what keeps that witch looking ...</td>\n",
       "      <td>2</td>\n",
       "      <td>sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-0</td>\n",
       "      <td>[USER] I wonder what keeps that witch looking ...</td>\n",
       "      <td>6</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-1</td>\n",
       "      <td>What do you guys think about female \"incels\"? ...</td>\n",
       "      <td>17</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-1</td>\n",
       "      <td>What do you guys think about female \"incels\"? ...</td>\n",
       "      <td>15</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.250661Z",
     "start_time": "2024-11-09T19:30:31.247369Z"
    }
   },
   "source": [
    "print(f\"There are: {len(data['annotator'].unique())} different annotators.\")\n",
    "print(\"Annotator IDs:\", sorted(data['annotator'].unique()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 19 different annotators.\n",
      "Annotator IDs: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18)]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 20000 unique comments was annotated by 3 different annotators. In 4444 cases, annotators reached a 2/3 agreement rather than full 3/3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.398481Z",
     "start_time": "2024-11-09T19:30:31.380694Z"
    }
   },
   "source": [
    "print(f\"There are: {len(data['rewire_id'].unique())} different comments annotated in total.\")\n",
    "print(\"Minimum number of annotations for a comment:\", data['rewire_id'].value_counts().min())\n",
    "print(\"Maximum number of annotations for a comment:\", data['rewire_id'].value_counts().max())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 20000 different comments annotated in total.\n",
      "Minimum number of annotations for a comment: 3\n",
      "Maximum number of annotations for a comment: 3\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.448636Z",
     "start_time": "2024-11-09T19:30:31.433627Z"
    }
   },
   "source": [
    "unique_label_counts = data.groupby('rewire_id')['label_sexist'].nunique() # 1 (3/3 agreement) or 2 (2/3 agreement)\n",
    "agreement_2_3_count = (unique_label_counts != 1).sum() # number of comments where annotators have 2/3 agreement\n",
    "\n",
    "print(\"Number of 'rewire_id' entries (comments) with 2/3 agreement among annotators:\", agreement_2_3_count)\n",
    "print(unique_label_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'rewire_id' entries (comments) with 2/3 agreement among annotators: 4444\n",
      "rewire_id\n",
      "sexism2022_english-0       2\n",
      "sexism2022_english-1       1\n",
      "sexism2022_english-10      2\n",
      "sexism2022_english-100     1\n",
      "sexism2022_english-1000    1\n",
      "                          ..\n",
      "sexism2022_english-9995    1\n",
      "sexism2022_english-9996    2\n",
      "sexism2022_english-9997    1\n",
      "sexism2022_english-9998    2\n",
      "sexism2022_english-9999    2\n",
      "Name: label_sexist, Length: 20000, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated dataset\n",
    "\n",
    "**Important note:** From now on in the pre-processing phase, we'll focus solely on the aggregated dataset. Processing the same comments 3 times and storing them in CoNLL-U format is unnecessary (additionally, we only need a single class label for our binary classification task later on). According to the paper describing the dataset, cases with 2/3 annotator agreement were resolved by the dataset creators. (If the aggregated dataset wasn‚Äôt available, we would likely rely on majority voting.)\n",
    "\n",
    "We'll keep in mind that 2/3 agreements exist and revisit this aspect during error analysis in milestone 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.555332Z",
     "start_time": "2024-11-09T19:30:31.508762Z"
    }
   },
   "source": [
    "data_agg = pd.read_csv('../data/edos_labelled_aggregated.csv')\n",
    "data_agg = data_agg.drop(columns=['label_category', 'label_vector'])\n",
    "print(\"Columns:\", list(data_agg.columns))\n",
    "print(\"Shape:\", data_agg.shape)\n",
    "data_agg"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['rewire_id', 'text', 'label_sexist', 'split']\n",
      "Shape: (20000, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                      rewire_id  \\\n",
       "0       sexism2022_english-9609   \n",
       "1      sexism2022_english-16993   \n",
       "2      sexism2022_english-13149   \n",
       "3      sexism2022_english-13021   \n",
       "4        sexism2022_english-966   \n",
       "...                         ...   \n",
       "19995   sexism2022_english-5228   \n",
       "19996  sexism2022_english-10140   \n",
       "19997   sexism2022_english-9726   \n",
       "19998  sexism2022_english-13365   \n",
       "19999   sexism2022_english-3523   \n",
       "\n",
       "                                                    text label_sexist  split  \n",
       "0      In Nigeria, if you rape a woman, the men rape ...   not sexist    dev  \n",
       "1                                Then, she's a keeper. üòâ   not sexist  train  \n",
       "2      This is like the Metallica video where the poo...   not sexist  train  \n",
       "3                                                 woman?   not sexist  train  \n",
       "4                         I bet she wished she had a gun   not sexist    dev  \n",
       "...                                                  ...          ...    ...  \n",
       "19995     girls really get fucked almost every weekend ?   not sexist  train  \n",
       "19996  The hatred for moslems are Reasonable and Just...   not sexist  train  \n",
       "19997                 Now this is a woman who gets it. üëÜ   not sexist  train  \n",
       "19998  ‚ÄúAmerican Idol‚Äù finalist [USER] said nothing i...   not sexist  train  \n",
       "19999      this bi--th should be stoped she's the rapist       sexist   test  \n",
       "\n",
       "[20000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_sexist</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-9609</td>\n",
       "      <td>In Nigeria, if you rape a woman, the men rape ...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-16993</td>\n",
       "      <td>Then, she's a keeper. üòâ</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-13149</td>\n",
       "      <td>This is like the Metallica video where the poo...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-13021</td>\n",
       "      <td>woman?</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-966</td>\n",
       "      <td>I bet she wished she had a gun</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>sexism2022_english-5228</td>\n",
       "      <td>girls really get fucked almost every weekend ?</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>sexism2022_english-10140</td>\n",
       "      <td>The hatred for moslems are Reasonable and Just...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>sexism2022_english-9726</td>\n",
       "      <td>Now this is a woman who gets it. üëÜ</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>sexism2022_english-13365</td>\n",
       "      <td>‚ÄúAmerican Idol‚Äù finalist [USER] said nothing i...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>sexism2022_english-3523</td>\n",
       "      <td>this bi--th should be stoped she's the rapist</td>\n",
       "      <td>sexist</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows √ó 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the train/test/dev split. Exploratory analysis will proceed on the combined training + validation sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.609183Z",
     "start_time": "2024-11-09T19:30:31.598868Z"
    }
   },
   "source": [
    "# split the data_agg into train, dev and test sets based on the 'split' column\n",
    "data_agg_train = data_agg[data_agg['split'] == 'train']\n",
    "data_agg_dev = data_agg[data_agg['split'] == 'dev']\n",
    "data_agg_test = data_agg[data_agg['split'] == 'test']\n",
    "\n",
    "print(\"Number of training samples:\", data_agg_train.shape[0]) \n",
    "print(\"Number of validation samples:\", data_agg_dev.shape[0]) \n",
    "print(\"Number of test samples:\", data_agg_test.shape[0]) \n",
    "\n",
    "assert data_agg_train.shape[0] + data_agg_dev.shape[0] + data_agg_test.shape[0] == data_agg.shape[0]\n",
    "\n",
    "# drop 'split' column\n",
    "data_agg_train = data_agg_train.drop('split', axis=1)\n",
    "data_agg_dev = data_agg_dev.drop('split', axis=1)\n",
    "data_agg_test = data_agg_test.drop('split', axis=1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14000\n",
      "Number of validation samples: 2000\n",
      "Number of test samples: 4000\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.653553Z",
     "start_time": "2024-11-09T19:30:31.649430Z"
    }
   },
   "source": [
    "data_agg_train_val = pd.concat([data_agg_train, data_agg_dev], axis=0, ignore_index=True)\n",
    "print(\"Shape of the training + validation set:\", data_agg_train_val.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training + validation set: (16000, 3)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions drawn using regular expressions:\n",
    "\n",
    "- `[URL]`, `[USER]` are placeholders used by dataset authors instead of actual URLs and real usernames\n",
    "- female related nouns and pronouns are more frequent than male\n",
    "- hashtags `#` often used\n",
    "- huge amount of profanities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.715009Z",
     "start_time": "2024-11-09T19:30:31.712040Z"
    }
   },
   "source": [
    "def count_patterns(pattern, data, top_k=20):\n",
    "    return Counter(match for text in data.text for match in re.findall(pattern, text)).most_common(top_k)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.789682Z",
     "start_time": "2024-11-09T19:30:31.764514Z"
    }
   },
   "source": [
    "count_patterns(r'\\[[A-Z]+\\]', data_agg_train_val) # catching: [USER], [URL]\n",
    "# count_patterns(r'\\b(she|her|wom[ae]n|female|girl|lady)\\b', data_agg_train_val) # female related nouns, pronouns etc.\n",
    "# count_patterns(r'\\b(he|him|his|m[ae]n|male|boy|guy|dude)\\b', data_agg_train_val) # male related nouns, pronouns etc.\n",
    "# count_patterns(r'#\\w+', data_agg_train_val) # hashtag\n",
    "# count_patterns(r'\\b(fuck|shit|damn|asshole|bitch|slut)\\b', data_agg_train_val) # profanities"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[URL]', 2004), ('[USER]', 1066), ('[DJT]', 1), ('[K]', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emojis\n",
    "\n",
    "One approach to identify emojis could be by looking for patterns that include `:` (e.g. we're targeting `:)`, `:(`, `:D`). However, modern text data often represents emojis differently, such as `√∞≈∏Àú‚Ä∞` for the üòâ (winking face) emoji. Python's `emoji` library is specifically designed for recognizing emojis in text and can replace them with descriptive names (e.g. `:winking_face:`), which is usefull for our application."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:31.852090Z",
     "start_time": "2024-11-09T19:30:31.847527Z"
    }
   },
   "source": [
    "print(\"Example of the comment with emoji: \\n\", data_agg_train_val.iloc[0]['text'])\n",
    "print(f\"Info about emoji: {data_agg_train_val.iloc[0]['text'][-1]} \\n {emoji.EMOJI_DATA[data_agg_train_val.iloc[0]['text'][-1]]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the comment with emoji: \n",
      " Then, she's a keeper. üòâ\n",
      "Info about emoji: üòâ \n",
      " {'en': ':winking_face:', 'status': 2, 'E': 0.6, 'alias': [':wink:']}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:32.069697Z",
     "start_time": "2024-11-09T19:30:31.908277Z"
    }
   },
   "source": [
    "def count_emojis(data, top_k=10):\n",
    "    return Counter([char for text in data.text for char in text if char in emoji.EMOJI_DATA]).most_common(top_k)\n",
    "\n",
    "count_emojis(data_agg_train_val)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üòÇ', 184),\n",
       " ('ü§î', 28),\n",
       " ('ü§£', 26),\n",
       " ('üòÅ', 20),\n",
       " ('üòä', 19),\n",
       " ('üòÑ', 18),\n",
       " ('‚ù§', 13),\n",
       " ('üëç', 13),\n",
       " ('üò∞', 12),\n",
       " ('üôÑ', 11)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:33.775991Z",
     "start_time": "2024-11-09T19:30:32.091141Z"
    }
   },
   "source": [
    "def replace_emojis_with_description(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "data_agg_train_val['text'] = data_agg_train_val['text'].apply(replace_emojis_with_description)\n",
    "data_agg_train_val.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  rewire_id  \\\n",
       "0  sexism2022_english-16993   \n",
       "1  sexism2022_english-13149   \n",
       "2  sexism2022_english-13021   \n",
       "3  sexism2022_english-14998   \n",
       "4   sexism2022_english-7228   \n",
       "\n",
       "                                                text label_sexist  \n",
       "0               Then, she's a keeper. :winking_face:   not sexist  \n",
       "1  This is like the Metallica video where the poo...   not sexist  \n",
       "2                                             woman?   not sexist  \n",
       "3  Unlicensed day care worker reportedly tells co...   not sexist  \n",
       "4  [USER] Leg day is easy. Hot girls who wear min...       sexist  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_sexist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-16993</td>\n",
       "      <td>Then, she's a keeper. :winking_face:</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-13149</td>\n",
       "      <td>This is like the Metallica video where the poo...</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-13021</td>\n",
       "      <td>woman?</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-14998</td>\n",
       "      <td>Unlicensed day care worker reportedly tells co...</td>\n",
       "      <td>not sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-7228</td>\n",
       "      <td>[USER] Leg day is easy. Hot girls who wear min...</td>\n",
       "      <td>sexist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:33.836925Z",
     "start_time": "2024-11-09T19:30:33.819616Z"
    }
   },
   "source": [
    "count_patterns(r':(.*?):', data_agg_train_val, top_k=10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('face_with_tears_of_joy', 183),\n",
       " ('thinking_face', 25),\n",
       " ('rolling_on_the_floor_laughing', 24),\n",
       " ('beaming_face_with_smiling_eyes', 20),\n",
       " ('grinning_face_with_smiling_eyes', 18),\n",
       " ('smiling_face_with_smiling_eyes', 18),\n",
       " ('United_States', 14),\n",
       " ('thumbs_up', 13),\n",
       " ('anxious_face_with_sweat', 12),\n",
       " ('face_with_rolling_eyes', 11)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization\n",
    "\n",
    "What are the most common words in our concatenated text?\n",
    "\n",
    "Examining the list of the most frequent words after tokenization and punctuation removal (`[]` were also highly frequent), we found that, alongside expected stopwords (`a`, `the` etc.), there is notable frequency of female-related nouns/pronouns (`her`, `she`, `women`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:37.028623Z",
     "start_time": "2024-11-09T19:30:33.864516Z"
    }
   },
   "source": [
    "words = [word for text in data_agg_train_val['text'] for word in nltk.word_tokenize(text)]\n",
    "words = [word for word in words if re.match(r'\\w', word)] # exclude punctuation\n",
    "print(\"Total number of words found after tokenization and punctuation removal:\", len(words))\n",
    "print(\"Top 20 most common words:\", Counter(words).most_common(20))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words found after tokenization and punctuation removal: 377897\n",
      "Top 20 most common words: [('the', 10148), ('a', 9499), ('to', 9329), ('I', 6935), ('and', 6905), ('you', 5517), ('is', 5361), ('of', 5279), ('her', 4632), ('that', 4290), ('she', 3975), ('in', 3803), ('it', 3607), ('women', 3346), ('for', 3159), (\"n't\", 3149), ('are', 2940), ('with', 2674), ('be', 2449), ('on', 2412)]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, let‚Äôs find the context surrounding the first few occurrences of some specific token of interest."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:37.089239Z",
     "start_time": "2024-11-09T19:30:37.082951Z"
    }
   },
   "source": [
    "def find_contexts(token, data, context_size=50, limit=10):\n",
    "    \"\"\"\n",
    "    Finds the first 'limit' occurrences of a given 'token' in dataframe 'data'\n",
    "    with a 'text' column, capturing 'context_size' characters around each occurrence.\n",
    "    Context: token position +- 'context_size' chars around.\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "\n",
    "    for text in data['text']:\n",
    "        for match in re.finditer(rf'\\b{token}\\b', text):\n",
    "            start, end = match.span()\n",
    "            # get the context around the match\n",
    "            context = text[max(0, start - context_size):min(len(text), end + context_size)] # +- context_size chars\n",
    "            contexts.append(context)\n",
    "            \n",
    "            if len(contexts) >= limit:\n",
    "                return contexts[:limit]\n",
    "\n",
    "    return contexts[:limit]\n",
    "\n",
    "find_contexts(\"women\", data_agg_train_val)\n",
    "# find_contexts(\"she\", data_agg_train_val)\n",
    "# find_contexts(\"yourself\", data_agg_train_val)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eed to stop referring to all men as kings and all women as queens, this shit is getting out of hand.',\n",
       " ' like 90% of people i know IRL it just so happens women are disgusted by me-and i dont even do it IRL ?',\n",
       " \"or Assault and based on his claim that he's raped women, he should be investigated. His bragging could be\",\n",
       " ' you have few options. You drop a few points with women doing that. A guy with options doesn‚Äôt give a shi',\n",
       " 'The she-god mentality of women is going down the drain. Men make the world work ',\n",
       " \"nt pussy. Sexbots will be realistic very soon and women's days of pussy rule will be over. &#x200B;\",\n",
       " ' Europe (France, Germany etc.) is easier and that women do approach men. Here they never approach, even c',\n",
       " 'Some hard core women haters use mgtow as an excuse, not a lifestyle.',\n",
       " 'that does not reflect poorly on men nor well upon women. It just means she is extraordinary, each sex has',\n",
       " 'Yeah, well, plenty of men have been hit by women without ever being able to hit back, so I am not ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopword removal\n",
    "\n",
    "`she`, `herself`, as well as `he`, `himself` etc. should be excluded from stopwords, because we assume that those pronouns provide relevant information for our task and preserving them will be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:37.133638Z",
     "start_time": "2024-11-09T19:30:37.129541Z"
    }
   },
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set = stopwords_set - {'she', 'she\\'s', 'herself', 'her', 'hers', 'he', 'himself', 'him', 'his', 'yourself', 'yourselves', 'your', 'yours'}\n",
    "print(sorted(stopwords_set))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'here', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\"]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stopword filtering and analyzing the most commonly occuring words, we can observe female-related nouns/pronounces, `URL`, `USER` tokens, as well as some profanities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:37.283720Z",
     "start_time": "2024-11-09T19:30:37.178382Z"
    }
   },
   "source": [
    "words = [word.lower() for word in words if word.lower() not in stopwords_set]\n",
    "print(\"Total number of words found after tokenization, punctuation removal and stopword filtering:\", len(words))\n",
    "print(\"Top 20 most common words:\", Counter(words).most_common(20))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words found after tokenization, punctuation removal and stopword filtering: 213597\n",
      "Top 20 most common words: [('she', 4884), ('her', 4749), ('women', 3580), (\"n't\", 3165), ('url', 2006), ('like', 1942), ('your', 1657), ('he', 1512), ('get', 1369), ('woman', 1363), ('would', 1253), ('men', 1236), ('user', 1073), ('one', 936), ('girls', 875), ('girl', 875), ('fuck', 874), ('his', 846), ('want', 799), ('think', 784)]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    "\n",
    "The process of reducing words to their base or dictionary form (lemma). We prefer lemmatization to stemming because it considers the word's context, making the reduction process more accurate. \n",
    "\n",
    "Let's just show one example..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:38.840250Z",
     "start_time": "2024-11-09T19:30:37.297134Z"
    }
   },
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma,pos') "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 20:30:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 37.8MB/s]                    \n",
      "2024-11-09 20:30:37 INFO: Downloaded file to /home/henry/stanza_resources/resources.json\n",
      "2024-11-09 20:30:37 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-09 20:30:37 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-09 20:30:37 INFO: Using device: cuda\n",
      "2024-11-09 20:30:37 INFO: Loading: tokenize\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:38 INFO: Loading: mwt\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:38 INFO: Loading: pos\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:38 INFO: Loading: lemma\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:30:39.065926Z",
     "start_time": "2024-11-09T19:30:38.852426Z"
    }
   },
   "source": [
    "text = data_agg_train_val.iloc[1]['text']\n",
    "doc = nlp(text) # process the text with the pipeline\n",
    "\n",
    "for sentence in doc.sentences[:1]:\n",
    "    for word in sentence.words[:5]: # show the 1st 5 words of the 1st sentence\n",
    "        print('\\t'.join([word.text, word.lemma, word.upos, word.feats if word.feats else '']))\n",
    "    print() "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tthis\tPRON\tNumber=Sing|PronType=Dem\n",
      "is\tbe\tAUX\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "like\tlike\tADP\t\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "Metallica\tMetallica\tPROPN\tNumber=Sing\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting in the CoNLL-U format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this part, we are converting the dataset into CoNLL-U format, which is commonly used for linguistic datasets. The CoNLL-U format lets us organize each token in a sentence with additional information, like its lemma, part-of-speech tag, and syntactic dependencies. Using Stanza‚Äôs NLP pipeline, we first preprocess every row in the dataset and then annotate it to include these features. In the function sentence_to_conllu_format, we turn each sentence into CoNLL-U structure and add some metadata for the label_sexist.\n",
    "\n",
    "After every row is converted, we use write_to_conllu function to save each dataset split (train, dev, test) as its own CoNLL-U file. This format is flexible and compatible with multiple NLP tools, which means we can use it in different steps of NLP processing. It also keeps detailed info for each token, which can be very useful for later model training and analysis.\n",
    "\n",
    "This code was executed on Google Colab using the GPU instance A100"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:47:52.950118Z",
     "start_time": "2024-11-09T19:30:39.079387Z"
    }
   },
   "source": [
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize custom stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set -= {'she', 'she\\'s', 'herself', 'her', 'hers', 'he', 'himself', 'him', 'his',\n",
    "                  'yourself', 'yourselves', 'your', 'yours'}\n",
    "logger.info(\"Customized stopwords list: %s\", sorted(stopwords_set))\n",
    "\n",
    "# Download and initialize Stanza English pipeline\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma,pos,depparse')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/edos_labelled_aggregated.csv')\n",
    "\n",
    "\n",
    "class TextProcessingPipeline:\n",
    "    \"\"\"Pipeline for processing text and converting it to CoNLL-U format.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_emojis_with_description(text):\n",
    "        \"\"\"Replace emojis in text with their descriptions.\"\"\"\n",
    "        return emoji.demojize(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_user_mentions_and_urls(text):\n",
    "        \"\"\"Remove user mentions and URLs and from text.\"\"\"\n",
    "        text = re.sub(r'\\[[A-Z]+\\]', '', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean text by converting to lowercase\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s.,!?\\'\"]+', '', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_stopwords(text):\n",
    "        \"\"\"Remove stopwords from text.\"\"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word not in stopwords_set]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Apply all preprocessing steps to text.\"\"\"\n",
    "        text = self.replace_emojis_with_description(text)\n",
    "        text = self.remove_user_mentions_and_urls(text)\n",
    "        text = self.clean_text(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        return text\n",
    "\n",
    "    def sentence_to_conllu_format(self, row):\n",
    "        \"\"\"Convert a sentence row to CoNLL-U format.\"\"\"\n",
    "        text = self.preprocess_text(row['text'])\n",
    "        sentence_id = row['rewire_id']\n",
    "        label_sexist = row['label_sexist']\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        conllu_format = [f\"# sent_id = {sentence_id}\",\n",
    "                         f\"# label_sexist = {label_sexist}\"]\n",
    "\n",
    "        for sentence in CoNLL.convert_dict(doc.to_dict()):\n",
    "            for token in sentence:\n",
    "                conllu_format.append('\\t'.join(str(field) for field in token))\n",
    "\n",
    "        return '\\n'.join(conllu_format)\n",
    "\n",
    "    def write_to_conllu(self, df, output_file):\n",
    "        \"\"\"Write the dataframe to a CoNLL-U formatted file.\"\"\"\n",
    "        total_rows = len(df)\n",
    "        with open(output_file, 'w') as f:\n",
    "            for _, row in tqdm(df.iterrows(), total=total_rows, desc=\"Processing rows\", ncols=100, leave=True):\n",
    "                conllu_sentence = self.sentence_to_conllu_format(row)\n",
    "                f.write(conllu_sentence + '\\n\\n')\n",
    "        logger.info(\"File saved: %s\", output_file)\n",
    "\n",
    "\n",
    "# Instantiating the pipeline\n",
    "pipeline = TextProcessingPipeline()\n",
    "\n",
    "# Splitting the data based on 'split' column\n",
    "data_agg_train = df[df['split'] == 'train'].drop('split', axis=1)\n",
    "data_agg_dev = df[df['split'] == 'dev'].drop('split', axis=1)\n",
    "data_agg_test = df[df['split'] == 'test'].drop('split', axis=1)\n",
    "\n",
    "# Logging the number of samples in each split\n",
    "logger.info(\"Number of training samples: %d\", data_agg_train.shape[0])\n",
    "logger.info(\"Number of validation samples: %d\", data_agg_dev.shape[0])\n",
    "logger.info(\"Number of test samples: %d\", data_agg_test.shape[0])\n",
    "\n",
    "# Writing each split to a separate CoNLL-U file\n",
    "pipeline.write_to_conllu(data_agg_train, 'train_sexism_dataset_conllu.conllu')\n",
    "pipeline.write_to_conllu(data_agg_dev, 'dev_sexism_dataset_conllu.conllu')\n",
    "pipeline.write_to_conllu(data_agg_test, 'test_sexism_dataset_conllu.conllu')\n",
    "\n",
    "logger.info(\n",
    "    \"Preprocessed datasets saved as 'train_sexism_dataset_conllu.conllu', 'dev_sexism_dataset_conllu.conllu', and 'test_sexism_dataset_conllu.conllu'.\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/henry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/henry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-11-09 20:30:39,090 - INFO - Customized stopwords list: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'here', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\"]\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 43.3MB/s]                    \n",
      "2024-11-09 20:30:39 INFO: Downloaded file to /home/henry/stanza_resources/resources.json\n",
      "2024-11-09 20:30:39,144 - INFO - Downloaded file to /home/henry/stanza_resources/resources.json\n",
      "2024-11-09 20:30:39 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-11-09 20:30:39,149 - INFO - Downloading default packages for language: en (English) ...\n",
      "2024-11-09 20:30:39 INFO: File exists: /home/henry/stanza_resources/en/default.zip\n",
      "2024-11-09 20:30:39,971 - INFO - File exists: /home/henry/stanza_resources/en/default.zip\n",
      "2024-11-09 20:30:42 INFO: Finished downloading models and saved to /home/henry/stanza_resources\n",
      "2024-11-09 20:30:42,800 - INFO - Finished downloading models and saved to /home/henry/stanza_resources\n",
      "2024-11-09 20:30:42 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "2024-11-09 20:30:42,802 - INFO - Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 31.9MB/s]                    \n",
      "2024-11-09 20:30:42 INFO: Downloaded file to /home/henry/stanza_resources/resources.json\n",
      "2024-11-09 20:30:42,870 - INFO - Downloaded file to /home/henry/stanza_resources/resources.json\n",
      "2024-11-09 20:30:42 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-09 20:30:42,876 - WARNING - Language en package default expects mwt, which has been added\n",
      "2024-11-09 20:30:43 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-11-09 20:30:43,346 - INFO - Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-11-09 20:30:43 INFO: Using device: cuda\n",
      "2024-11-09 20:30:43,347 - INFO - Using device: cuda\n",
      "2024-11-09 20:30:43 INFO: Loading: tokenize\n",
      "2024-11-09 20:30:43,348 - INFO - Loading: tokenize\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:43 INFO: Loading: mwt\n",
      "2024-11-09 20:30:43,355 - INFO - Loading: mwt\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:43 INFO: Loading: pos\n",
      "2024-11-09 20:30:43,359 - INFO - Loading: pos\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:43 INFO: Loading: lemma\n",
      "2024-11-09 20:30:43,593 - INFO - Loading: lemma\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:43 INFO: Loading: depparse\n",
      "2024-11-09 20:30:43,718 - INFO - Loading: depparse\n",
      "/home/henry/Documents/SharedDocuments/Uni/TU/3.Semester/NLP/.venv/lib/python3.9/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-09 20:30:43 INFO: Done loading processors!\n",
      "2024-11-09 20:30:43,925 - INFO - Done loading processors!\n",
      "2024-11-09 20:30:43,968 - INFO - Number of training samples: 14000\n",
      "2024-11-09 20:30:43,968 - INFO - Number of validation samples: 2000\n",
      "2024-11-09 20:30:43,969 - INFO - Number of test samples: 4000\n",
      "Processing rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14000/14000 [10:20<00:00, 22.57it/s]\n",
      "2024-11-09 20:41:04,290 - INFO - File saved: train_sexism_dataset_conllu.conllu\n",
      "Processing rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [02:36<00:00, 12.75it/s]\n",
      "2024-11-09 20:43:41,158 - INFO - File saved: dev_sexism_dataset_conllu.conllu\n",
      "Processing rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [04:11<00:00, 15.89it/s]\n",
      "2024-11-09 20:47:52,947 - INFO - File saved: test_sexism_dataset_conllu.conllu\n",
      "2024-11-09 20:47:52,948 - INFO - Preprocessed datasets saved as 'train_sexism_dataset_conllu.conllu', 'dev_sexism_dataset_conllu.conllu', and 'test_sexism_dataset_conllu.conllu'.\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
